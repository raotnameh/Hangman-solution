{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "from charbert import *\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "\n",
    "# Create the model\n",
    "vocab_size=len(tokenizer.vocab)\n",
    "model = CNNBERT(vocab_size=vocab_size)\n",
    "\n",
    "# use cuda if available\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pth\"), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location) \n",
    "        self.full_dictionary = list( set(self.full_dictionary) )\n",
    "        \n",
    "        # updated the prob with length adjusted. \n",
    "        self.ngrams = 3\n",
    "        self.Ngramdict = self.ngram_prob(self.full_dictionary, self.ngrams)\n",
    "        self.unigram = sorted( [(k,v) for k,v in self.Ngramdict['gram_1'].items()], key = lambda x: x[1], reverse = True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            requests.get(link)\n",
    "\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def ngram_prob(self,full_dictionary, Ngrams = 6):\n",
    "        \"\"\"\n",
    "        Generate the Ngram probabilities for the dictionary\n",
    "\n",
    "        Args:\n",
    "            full_dictionary (_type_): _description_\n",
    "            Ngrams (int, optional): _description_. Defaults to 6.\n",
    "        \"\"\"\n",
    "        def generate_N_grams(text,ngram=1):\n",
    "            char = [i for i in text]\n",
    "\n",
    "            temp=zip(*[char[i:] for i in range(0,ngram)])\n",
    "            ans=[' '.join(ngram) for ngram in temp]\n",
    "            return ans\n",
    "        \n",
    "        Ngramdict = {f\"gram_{n}\": defaultdict(int) for n in range(1,Ngrams+1)}\n",
    "        FNgramdict = {}\n",
    "\n",
    "        for gram,dict in Ngramdict.items():\n",
    "            for word in full_dictionary:\n",
    "                for ngram in generate_N_grams(word,int(gram[-1])):\n",
    "                    dict[ngram]+=1\n",
    "            # normalize the ngram counts\n",
    "            sum_ = sum(dict.values())\n",
    "            FNgramdict[gram] = {k: np.log(v/sum_) for k,v in dict.items()}\n",
    "        \n",
    "        return FNgramdict\n",
    "\n",
    "    def calculate_probability_chain_rule(self, word):\n",
    "        \"\"\"\n",
    "        Calculate the probability of a word using the chain rule\n",
    "\n",
    "        Args:\n",
    "            word: word to calculate the probability of\n",
    "\n",
    "        Returns:\n",
    "            log_probability: the log probability of the word\n",
    "        \"\"\"\n",
    "        Ngramdict = self.Ngramdict\n",
    "        # Convert word to a list of characters and initialize the chain\n",
    "        chain = [\"\".join(word[:i])[-self.ngrams:] for i in range(1, len(word) + 1)]\n",
    "        \n",
    "        # calculate the log probability of the word using the chain rule\n",
    "        log_probability = 0\n",
    "        for c in chain[::-1]:\n",
    "            use_n = min(self.ngrams, len(c))  # Determine the appropriate N-gram length\n",
    "            c = \" \".join(c)  # Format the chain segment as a space-separated string\n",
    "            \n",
    "            gram_key = f\"gram_{use_n}\"\n",
    "            if c in Ngramdict[gram_key]:\n",
    "                log_probability += Ngramdict[gram_key][c]\n",
    "            else: \n",
    "                log_probability += 2*min( Ngramdict[gram_key].values())\n",
    "\n",
    "        return log_probability\n",
    "    \n",
    "    def scaledcharprob(self, given_dictionary):\n",
    "        \"\"\"\n",
    "        Get the character probabilities scaled using the scores\n",
    "\n",
    "        Args:\n",
    "            given_dictionary: dictionary of words and their scores\n",
    "\n",
    "        Returns:\n",
    "            char_prob: list of characters and their probabilities\n",
    "        \"\"\"\n",
    "        listofchar = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z')\n",
    "        \n",
    "        \n",
    "        scores = [i[1] for i in given_dictionary]\n",
    "        scores = np.array(scores)[:,None] # beam, 1\n",
    "        assert scores.shape == (len(given_dictionary),1)\n",
    "\n",
    "        words = [i[0] for i in given_dictionary]\n",
    "        if len(words) == 0:\n",
    "            return [(i, 1/len(listofchar)) for i in listofchar]\n",
    "        \n",
    "        \n",
    "        char_prob = []\n",
    "        for word in words: \n",
    "            dummy = [0]*len(listofchar)\n",
    "            for char in word:\n",
    "                dummy[listofchar.index(char)] += 1\n",
    "         \n",
    "            dummy = [x / len(word) for x in dummy]\n",
    "            # smoothing \n",
    "            dummy = [i + max(dummy)*0.01 for i in dummy]\n",
    "            dummy = [i/sum(dummy) for i in dummy]\n",
    "            char_prob.append(dummy)\n",
    "        \n",
    "        char_prob = np.array(char_prob)\n",
    "        char_prob = np.log(char_prob) # beam, 26\n",
    "        \n",
    "        char_prob += scores\n",
    "        char_prob /= char_prob.shape[0]\n",
    "        \n",
    "        char_prob = char_prob.sum(axis=0)\n",
    "\n",
    "        length = len(char_prob)\n",
    "        char_prob = [(listofchar[i], char_prob[i]) for i in range(length)]\n",
    "        char_prob.sort(key = lambda x: x[1], reverse = True)        \n",
    "\n",
    "        return char_prob\n",
    "    \n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"[::2].replace(\"_\",\".\")\n",
    "                  \n",
    "        def likelihhod_charbert(word,topk=4): # using character bert model \n",
    "            \"\"\"\n",
    "            Get the likelihood of the possible words using the character bert model\n",
    "\n",
    "            Args:\n",
    "                word: word to get the likelihood of\n",
    "                topk: number of top k possible words to return\n",
    "\n",
    "            Returns:\n",
    "                possible_words: dictionary of possible words and their likelihood scores\n",
    "            \"\"\"\n",
    "\n",
    "            ratio = 1 - len(word.replace(\"_\",\"\"))/len(word)\n",
    "            topk = max(1, int(topk*ratio))\n",
    "            word = word[::2]\n",
    "            \n",
    "            possible_words = Test(model, tokenizer, device, topk, word, self.guessed_letters)\n",
    "            possible_words = {i[0]:i[1] for i in possible_words} # log probability of the words\n",
    "       \n",
    "            return possible_words\n",
    "\n",
    "        def posterior(word):\n",
    "            \"\"\"\n",
    "            Calculate the posterior of the possible words using the likelihood and prior\n",
    "\n",
    "            Args:\n",
    "                word: mask word to calculate the posterior of\n",
    "\n",
    "            Returns:\n",
    "                scaledcharprob: list of characters and their probabilities\n",
    "            \"\"\"\n",
    "            \n",
    "            # word statistics\n",
    "            num_masked = word.count(\"_\")\n",
    "            len_word = len(word[::2])\n",
    "            \n",
    "            # step 1: if all the characters are masked, return the unigram\n",
    "            if num_masked == len_word : \n",
    "                return self.unigram \n",
    "            \n",
    "            # step 2: if some characters are masked, return the posterior using the likelihood and prior\n",
    "            \n",
    "            # charbert_likelihood\n",
    "            self.likelihood = likelihhod_charbert(word)\n",
    "            \n",
    "            # Use the ngrams to get the Prior over the possible words generated by the charbert model\n",
    "            dummy = {}\n",
    "            for k in self.likelihood.keys():\n",
    "                dummy[k] = self.calculate_probability_chain_rule(k)\n",
    "            self.Prior = dummy\n",
    "            \n",
    "            # rescale the likelihood and prior to the same scale\n",
    "            max_likelihood = max(self.likelihood.values())\n",
    "            max_prior = max(self.Prior.values())\n",
    "            rescale = max_likelihood / max_prior\n",
    "            self.Prior = {k: v*rescale for k,v in self.Prior.items()}\n",
    "        \n",
    "            # posterior\n",
    "            self.Posterior = {}\n",
    "            for k in self.likelihood.keys():\n",
    "                self.Posterior[k] = self.likelihood[k] + self.Prior[k]\n",
    "                \n",
    "            self.Posterior = [(k,v) for k,v in self.Posterior.items()]\n",
    "            return self.scaledcharprob(self.Posterior)\n",
    "        \n",
    "        def Guess(dist):\n",
    "            \"\"\"\n",
    "            Guess the letter to fill in the word\n",
    "\n",
    "            Args:\n",
    "                dist: distribution of the possible characters\n",
    "\n",
    "            Returns:\n",
    "                guess_letter: the most probable character to fill in the masked word\n",
    "            \"\"\"\n",
    "            dist.sort(key = lambda x: x[1], reverse = True) # sort the distribution in descending order\n",
    "                 \n",
    "            for letter,_ in dist:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    guess_letter = letter\n",
    "                    break            \n",
    "                    \n",
    "            return guess_letter\n",
    "        \n",
    "        guess_letter = Guess(posterior(word))    \n",
    "        \n",
    "        return guess_letter\n",
    "\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        # Add `access_token` to post_args or args if it has not already been\n",
    "        # included.\n",
    "        if self.access_token:\n",
    "            # If post_args exists, we assume that args either does not exists\n",
    "            # or it does not need `access_token`.\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HangmanAPI(access_token=\"f718c442ecd42d896508fe7db1789a\", timeout=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.start_game(practice=1,verbose=True)\n",
    "\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))\n",
    "\n",
    "total_practice_successes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.530\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
